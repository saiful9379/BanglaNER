{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bangla NER\n",
        "Bangla Name Entity Recognition(NER) using SpaCy. this model training using PER(PERSON) class. the main purpose extract human name from text string."
      ],
      "metadata": {
        "id": "QltRagoDp-zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "2BVMfvbIpsIa",
        "outputId": "ba3b5a11-30e7-4051-fbf2-e85d3fa64982",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.10/dist-packages (1.3.3)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (3.7.2)\n",
            "Requirement already satisfied: transformers<4.36.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.4.8)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (1.23.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.1.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy-transformers) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy-transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the base config file to directory. For more information please check spacy"
      ],
      "metadata": {
        "id": "Q-v7BAMHqto4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile base_config.cfg\n",
        "# This is an auto-generated partial config. To use it with 'spacy train'\n",
        "# you can run spacy init fill-config to auto-fill all default settings:\n",
        "# python -m spacy init fill-config ./base_config.cfg ./config.cfg\n",
        "[paths]\n",
        "train = null\n",
        "dev = null\n",
        "vectors = null\n",
        "[system]\n",
        "gpu_allocator = \"pytorch\"\n",
        "\n",
        "[nlp]\n",
        "lang = \"bn\"\n",
        "pipeline = [\"transformer\",\"ner\"]\n",
        "batch_size = 256\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.transformer]\n",
        "factory = \"transformer\"\n",
        "\n",
        "[components.transformer.model]\n",
        "@architectures = \"spacy-transformers.TransformerModel.v3\"\n",
        "name = \"csebuetnlp/banglabert\"\n",
        "tokenizer_config = {\"use_fast\": true}\n",
        "# max_length = 512\n",
        "\n",
        "[components.transformer.model.get_spans]\n",
        "@span_getters = \"spacy-transformers.strided_spans.v1\"\n",
        "window = 256\n",
        "stride = 96\n",
        "\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "\n",
        "[components.ner.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"ner\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 64\n",
        "maxout_pieces = 2\n",
        "use_upper = false\n",
        "nO = null\n",
        "\n",
        "[components.ner.model.tok2vec]\n",
        "@architectures = \"spacy-transformers.TransformerListener.v1\"\n",
        "grad_factor = 1.0\n",
        "\n",
        "[components.ner.model.tok2vec.pooling]\n",
        "@layers = \"reduce_mean.v1\"\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "\n",
        "[training]\n",
        "accumulate_gradient = 3\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "\n",
        "[training.optimizer.learn_rate]\n",
        "@schedules = \"warmup_linear.v1\"\n",
        "warmup_steps = 250\n",
        "total_steps = 20000\n",
        "initial_rate = 5e-5\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_padded.v1\"\n",
        "discard_oversize = true\n",
        "size = 2000\n",
        "buffer = 256\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}"
      ],
      "metadata": {
        "id": "jHab0Eitp8bp",
        "outputId": "6d124f9e-56f5-4a1a-d319-0b6e3a65ae74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing base_config.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert Base config to training config file"
      ],
      "metadata": {
        "id": "aeGumw9KrAYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config ./base_config.cfg ./config.cfg"
      ],
      "metadata": {
        "id": "2cNQIQeZqshI",
        "outputId": "b8e8904a-2d29-4352-c21c-a66ba1807861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-08 09:05:01.836139: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-08 09:05:01.836199: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-08 09:05:01.836235: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-08 09:05:01.843977: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-08 09:05:02.923400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-12-08 09:05:04.315105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-08 09:05:04.315615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-08 09:05:04.315815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLIOU and BIO format are similar type data format for NER.\n",
        "\n",
        "BLIOU data format:\n",
        "\n",
        "    B = Begin\n",
        "\n",
        "    L = Last\n",
        "\n",
        "    I = Inside\n",
        "\n",
        "    O = Outside\n",
        "\n",
        "    U = Unique\n",
        "\n",
        "IOB data format:\n",
        "\n",
        "    I = Inside\n",
        "\n",
        "    O = Outside\n",
        "\n",
        "    B = Begin"
      ],
      "metadata": {
        "id": "7Blf8dB-rJLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wget"
      ],
      "metadata": {
        "id": "8jsOKNgRrDa3",
        "outputId": "e836f22f-4f66-404b-94ea-8de54a434750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=4fa4e0c8a6bbe578aaeda5e117ee16fc78591c2e2ef845bae0b4b242b8f82bc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Data from huggingface, for more information [check](https://huggingface.co/datasets/saiful9379/BanglaNER_V1.0)"
      ],
      "metadata": {
        "id": "yTtaUA9Ruf98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wget\n",
        "\n",
        "\n",
        "def download_data_form_huggingface(url, output_dir):\n",
        "    wget.download(url, out=output_dir)\n",
        "\n",
        "data = \"data\"\n",
        "os.makedirs(data, exist_ok=True)"
      ],
      "metadata": {
        "id": "P2mone19tejs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = \"https://huggingface.co/datasets/saiful9379/BanglaNER_V1.0/resolve/main/train.jsonl?download=true\"\n",
        "val_data = \"https://huggingface.co/datasets/saiful9379/BanglaNER_V1.0/resolve/main/val.jsonl?download=true\""
      ],
      "metadata": {
        "id": "rQo1ue28ue5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_data_form_huggingface(train_data, data)"
      ],
      "metadata": {
        "id": "wroSrYj0uaZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_data_form_huggingface(val_data, data)"
      ],
      "metadata": {
        "id": "Fsxyvkx_ubAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert docanno format to spacy format"
      ],
      "metadata": {
        "id": "I8LtiU1vvaGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "VISUALIZATION_STATUS = False\n",
        "\n",
        "\n",
        "def save_jsonl(data, output_dir):\n",
        "    \"\"\"\n",
        "    Save a list of data into a JSON Lines file.\n",
        "\n",
        "    Parameters:\n",
        "        - data (list): List of data to be saved.\n",
        "        - filename (str): The name of the file to save.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    with open(output_dir, 'w', encoding='utf-8') as file:\n",
        "        for item in data:\n",
        "            json.dump(item, file, ensure_ascii=False)\n",
        "            file.write('\\n')\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = f.read().split(\"\\n\")\n",
        "    data = [json.loads(line) for line in data if line]\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def convert_spacy_format(data, output_path=\"data/unkonwn.spacy\"):\n",
        "    \"\"\"\n",
        "    Convert data to Spacy format and save it as a DocBin.\n",
        "\n",
        "    Parameters:\n",
        "        - data (list): List of tuples containing text and annotations.\n",
        "        - output_path (str): The path to save the Spacy DocBin file.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    nlp = spacy.blank(\"bn\") # load a new spacy model\n",
        "    db = DocBin() # create a DocBin object\n",
        "    number_of_skip_entity, processed_line = 0, 0\n",
        "    for text, annot in tqdm(data): # data in previous format\n",
        "        try:\n",
        "            doc = nlp.make_doc(text) # create doc object from text\n",
        "            ents = []\n",
        "            for start, end, label in annot[\"entities\"]: # add character indexes\n",
        "\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n",
        "                # print(start, end, label, span)\n",
        "                if span is None:\n",
        "                    s = doc.text\n",
        "                    sub_E = s[end:]\n",
        "                    sub_S = s[:start]\n",
        "                    end = end+ (0 if len(sub_E.split(\" \", 1)[0]) <= 0 else len(sub_E.split(\" \", 1)[0]))\n",
        "                    start = start - (0 if len(sub_S.rsplit(\" \", 1)[-1]) <= 0 else len(sub_S.rsplit(\" \", 1)[-1]))\n",
        "\n",
        "                    span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n",
        "                    if span is None:\n",
        "                        number_of_skip_entity += 1\n",
        "                        # print(\"++++++++++++++++++++++++++++Skipping entity Start++++++++++++++++++++++++++++\")\n",
        "                        # print(start, end, label, span)\n",
        "                        # print(doc.text[start:end],doc.text[start],doc.text[end],'kh',sep='|')\n",
        "                        # print(\"++++++++++++++++++++++++++++Skipping entity End++++++++++++++++++++++++++++++\")\n",
        "                        break\n",
        "                else:\n",
        "                    processed_line += 1\n",
        "                    ents.append(span)\n",
        "            doc.ents = ents # label the text with the ents\n",
        "            if VISUALIZATION_STATUS:\n",
        "                spacy.displacy.render(doc, style=\"ent\", jupyter=True)\n",
        "            db.add(doc)\n",
        "        except:\n",
        "            number_of_skip_entity += 1\n",
        "    db.to_disk(output_path) # save the docbin object\n",
        "\n",
        "    print(f\" Spacy Processed file   : {output_path}\")\n",
        "    print(f\" No. of Processed line : {processed_line}\")\n",
        "    print(f\" No. of Skip Entity  : {number_of_skip_entity}\")\n",
        "\n",
        "\n",
        "\n",
        "def data_convert_spacy_format(file_path):\n",
        "    \"\"\"\n",
        "    Convert data from a JSON Lines file to Spacy format.\n",
        "\n",
        "    Parameters:\n",
        "        - jsonl_file_path (str): The path to the JSON Lines file.\n",
        "\n",
        "    Returns:\n",
        "        - training_data (list): List of tuples containing text and annotations.\n",
        "    \"\"\"\n",
        "    # need to check empty line\n",
        "    training_data, lines=[], []\n",
        "    # with open(jsonl_file_path, 'r') as f:\n",
        "    #     data = f.read().split(\"\\n\")\n",
        "    # # print(data)\n",
        "    # for line in data:\n",
        "    #     j_line = json.loads(line)\n",
        "    data = read_jsonl_file(file_path)\n",
        "    for line in data:\n",
        "        text, entities= line['text'], line['label']\n",
        "        if len(entities)>0:\n",
        "            training_data.append((text, {\"entities\" : entities}))\n",
        "    return training_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import glob\n",
        "input_dir = \"./data\"\n",
        "output_dir = \"./data\"\n",
        "\n",
        "jsonl_files = glob.glob(input_dir+\"/*.jsonl\")\n",
        "for jsonl_file in jsonl_files:\n",
        "    file_name = os.path.basename(jsonl_file).split(\".\")[0]+\".spacy\"\n",
        "    data = data_convert_spacy_format(jsonl_file)\n",
        "    convert_spacy_format(data, output_path=os.path.join(output_dir, file_name))"
      ],
      "metadata": {
        "id": "-8ofbj43u-YM",
        "outputId": "95a04c1c-0663-4686-acf5-ee23238f5d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4483/4483 [00:02<00:00, 2161.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Spacy Processed file   : ./data/train.spacy\n",
            " No. of Processed line : 4447\n",
            " No. of Skip Entity  : 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1161/1161 [00:00<00:00, 1256.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Spacy Processed file   : ./data/val.spacy\n",
            " No. of Processed line : 1146\n",
            " No. of Skip Entity  : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training BanglaNER model"
      ],
      "metadata": {
        "id": "O2YQ8TpSwsDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy train config.cfg \\\n",
        "    --gpu-id 0 \\\n",
        "    --output ./models/bangla_ner_model \\\n",
        "    --paths.train ./data/train.spacy \\\n",
        "    --paths.dev ./data/train.spacy"
      ],
      "metadata": {
        "id": "KMWhnCHsvRUE",
        "outputId": "8e9921ab-0c06-4d83-de93-4f9547f4fac5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-08 09:28:36.176569: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-08 09:28:36.176625: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-08 09:28:36.176659: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-08 09:28:37.248824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: models/bangla_ner_model\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: models/bangla_ner_model\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "tokenizer_config.json: 100% 119/119 [00:00<00:00, 631kB/s]\n",
            "config.json: 100% 586/586 [00:00<00:00, 4.10MB/s]\n",
            "vocab.txt: 100% 528k/528k [00:00<00:00, 2.66MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 716kB/s]\n",
            "pytorch_model.bin: 100% 443M/443M [00:23<00:00, 18.7MB/s]\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0         195.18    201.66   13.47    8.78   28.94    0.13\n",
            "  3     200       31138.61  37226.07   92.14   90.38   93.96    0.92\n",
            "  6     400        1903.80   2335.38   96.53   97.07   95.99    0.97\n",
            "  9     600        1054.23   1168.81   97.63   99.06   96.24    0.98\n",
            " 12     800         811.43    945.38   98.00   99.25   96.79    0.98\n",
            " 15    1000         724.04    815.67   98.18   99.72   96.70    0.98\n",
            " 18    1200         707.10    773.46   98.19   98.80   97.58    0.98\n",
            " 21    1400         725.14    757.95   98.22   99.63   96.86    0.98\n",
            " 24    1600         730.77    738.04   98.20   99.67   96.76    0.98\n",
            " 27    1800         743.11    732.03   98.20   99.51   96.92    0.98\n",
            " 30    2000         715.43    682.28   98.16   99.58   96.79    0.98\n",
            " 33    2200         724.05    684.08   98.21   99.67   96.79    0.98\n",
            " 36    2400         740.46    701.62   98.18   99.35   97.04    0.98\n",
            " 39    2600         742.81    696.84   98.22   98.60   97.84    0.98\n",
            " 43    2800         733.48    678.28   98.23   99.74   96.76    0.98\n",
            " 46    3000         699.54    658.81   98.22   99.69   96.79    0.98\n",
            " 49    3200         710.26    657.94   98.14   99.74   96.58    0.98\n",
            " 52    3400         773.77    691.48   98.26   99.56   96.99    0.98\n",
            " 55    3600         708.33    653.60   98.25   99.77   96.79    0.98\n",
            " 58    3800         746.62    683.22   98.24   98.58   97.90    0.98\n",
            " 61    4000         736.53    651.05   98.27   98.74   97.81    0.98\n",
            " 64    4200         688.89    647.04   98.21   98.51   97.90    0.98\n",
            " 67    4400         743.88    685.15   98.22   99.49   96.99    0.98\n",
            " 70    4600         663.82    626.35   98.22   98.98   97.47    0.98\n",
            " 73    4800         694.23    640.27   98.26   99.32   97.22    0.98\n",
            " 76    5000         710.29    667.76   98.24   99.79   96.74    0.98\n",
            " 79    5200         673.45    638.85   98.27   98.76   97.79    0.98\n",
            " 82    5400         670.44    648.85   98.26   99.19   97.36    0.98\n",
            " 85    5600         669.18    637.75   98.27   98.69   97.86    0.98\n",
            " 88    5800         665.81    642.80   98.25   98.82   97.68    0.98\n",
            " 91    6000         623.87    607.30   98.22   99.67   96.81    0.98\n",
            " 94    6200         657.77    630.54   98.24   98.53   97.95    0.98\n",
            " 97    6400         684.62    638.76   98.23   99.72   96.79    0.98\n",
            "101    6600         692.02    657.61   98.25   98.67   97.84    0.98\n",
            "104    6800         630.32    617.74   98.24   99.72   96.81    0.98\n",
            "107    7000         719.14    607.53   98.23   98.53   97.93    0.98\n",
            "110    7200         645.50    629.59   98.27   98.69   97.86    0.98\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "models/bangla_ner_model/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "jpC8vEZIwyC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U spacy\n",
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"models/bangla_ner_model/model-best\")\n",
        "\n",
        "text_list = [\n",
        "    \"আব্দুর রহিম নামের কাস্টমারকে একশ টাকা বাকি দিলাম\",\n",
        "    \"নতুন বছরে জ্বলছেন আরও একজন—রজার ফেদেরার ।\",\n",
        "    \"ডিপিডিসির স্পেশাল টাস্কফোর্সের প্রধান মুনীর চৌধুরী জানান\",\n",
        "    \"তিনি মোহাম্মদ বাকির আল-সদর এর ছাত্র ছিলেন।\",\n",
        "    \"লিশ ট্র্যাক তৈরির সময় বেশ কয়েকজন শিল্পীর দ্বারা অনুপ্রাণিত হওয়ার কথা স্মরণ করেন, বিশেষ করে ফ্রাঙ্ক সিনাত্রা ।\",\n",
        "]\n",
        "for text in text_list:\n",
        "    doc = nlp(text)\n",
        "    print(f\"Input: {text}\")\n",
        "    for entity in doc.ents:\n",
        "        print(f\"Entity: {entity.text}, Label: {entity.label_}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "SE3mymnHwjZO",
        "outputId": "a034376f-53a1-414f-8acf-514851b6066f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: আব্দুর রহিম নামের কাস্টমারকে একশ টাকা বাকি দিলাম\n",
            "Entity: আব্দুর রহিম, Label: PER\n",
            "---\n",
            "Input: নতুন বছরে জ্বলছেন আরও একজন—রজার ফেদেরার ।\n",
            "---\n",
            "Input: ডিপিডিসির স্পেশাল টাস্কফোর্সের প্রধান মুনীর চৌধুরী জানান\n",
            "Entity: মুনীর চৌধুরী, Label: PER\n",
            "---\n",
            "Input: তিনি মোহাম্মদ বাকির আল-সদর এর ছাত্র ছিলেন।\n",
            "Entity: মোহাম্মদ বাকির আল-সদর, Label: PER\n",
            "---\n",
            "Input: লিশ ট্র্যাক তৈরির সময় বেশ কয়েকজন শিল্পীর দ্বারা অনুপ্রাণিত হওয়ার কথা স্মরণ করেন, বিশেষ করে ফ্রাঙ্ক সিনাত্রা ।\n",
            "Entity: ফ্রাঙ্ক সিনাত্রা, Label: PER\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2pl3gvAD5F_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}